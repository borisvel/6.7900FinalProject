{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "bd64b950",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import multivariate_normal as mvn\n",
    "import random\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "import torchvision.datasets as dset\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "132541fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 200\n",
    "batch_size = 64\n",
    "lr = 1e-4\n",
    "b1 = 0.5\n",
    "b2 = 0.999\n",
    "n_cpu = 8\n",
    "latent_dim = 40\n",
    "img_size = 64\n",
    "channels = 3\n",
    "sample_interval=400\n",
    "lam1 = 1e-1\n",
    "lam2 = 1e-2\n",
    "lam3 = 5\n",
    "sample_pixels = sorted(random.sample(list(range(img_size*img_size*3)), 6500))\n",
    "\n",
    "img_shape = (channels, img_size, img_size)\n",
    "\n",
    "cuda = True if torch.cuda.is_available() else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "1aaa4530",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataroot = \"resized_data_bayc_no_background\"\n",
    "dataset = dset.ImageFolder(root=dataroot,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Resize(img_size),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                           ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "77e53c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 5000\n",
    "image_samples = np.zeros((samples, img_size*img_size*3))\n",
    "for i in range(0, samples):\n",
    "    sample_idx = torch.randint(len(dataset), size=(1,)).item()\n",
    "    img, _ = dataset[sample_idx]\n",
    "    flat = torch.flatten(img).numpy()\n",
    "    image_samples[i] = flat\n",
    "image_samples_apes = image_samples.T[sample_pixels]\n",
    "image_samples_apes = torch.tensor(image_samples_apes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "79014a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean_apes = np.mean(image_samples_apes, axis = 1)\n",
    "#cov_apes = np.cov(image_samples_apes) + 0.0001*np.identity(len(sample_pixels)) \n",
    "#dist_apes = mvn(mean_apes, cov_apes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "8f96eb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataroot = \"resized_data_unicorns\"\n",
    "dataset = dset.ImageFolder(root=dataroot,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Resize(img_size),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                           ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "e6ff340d",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 1100\n",
    "image_samples = np.zeros((samples, img_size*img_size*3))\n",
    "for i in range(0, samples):\n",
    "    sample_idx = torch.randint(len(dataset), size=(1,)).item()\n",
    "    img, _ = dataset[sample_idx]\n",
    "    flat = torch.flatten(img).numpy()\n",
    "    image_samples[i] = flat\n",
    "image_samples_unicorns = image_samples.T[sample_pixels]\n",
    "image_samples_unicorns = torch.tensor(image_samples_unicorns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "6ef5e501",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean_unicorns = np.mean(image_samples_unicorns, axis = 1)\n",
    "#cov_unicorns = np.cov(image_samples_unicorns) + 0.0001*np.identity(len(sample_pixels)) \n",
    "#dist_unicorns = mvn(mean_unicorns, cov_unicorns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "b8a53bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Linear(in_feat, out_feat)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *block(latent_dim, 128, normalize=False),\n",
    "            *block(128, 256),\n",
    "            *block(256, 512),\n",
    "            *block(512, 1024),\n",
    "            nn.Linear(1024, int(np.prod(img_shape))),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def regularization(self, gen_imgs):\n",
    "        \n",
    "        result1 = 0\n",
    "        result2 = 0\n",
    "        sample_images = random.sample(list(range(len(gen_imgs))), 5)\n",
    "        for i in sample_images:\n",
    "            img = gen_imgs[i]\n",
    "            flat = torch.flatten(img)[sample_pixels]\n",
    "            a = torch.log(torch.min(torch.linalg.norm(torch.transpose(torch.transpose(image_samples_apes, 0, 1) - flat.expand(5000, len(sample_pixels)), 0, 1),dim=0)))\n",
    "            #b = torch.log(torch.min(torch.linalg.norm(torch.transpose(torch.transpose(image_samples_unicorns, 0, 1) - flat.expand(1100, len(sample_pixels)), 0, 1),dim=0)))\n",
    "            result1 += -a\n",
    "            #result2 += -b\n",
    "        return lam1*result1/len(sample_images) + lam2*result2/len(sample_images)\n",
    "        \n",
    "    def regularization_fast(self, gen_imgs, autoencoder):\n",
    "        result = 0\n",
    "        for img in gen_imgs:\n",
    "            enc = torch.flatten(autoencoder.encode(img.view(1, *img.shape)))\n",
    "            a = torch.log(torch.min(torch.linalg.norm(torch.transpose(torch.transpose(image_sample_apes_encoded, 0, 1) - enc.expand(1868, 32), 0, 1),dim=0)))\n",
    "            result += -a\n",
    "        return lam1*result/len(gen_imgs)\n",
    "            \n",
    "    def regularization2(self, classifier, gen_imgs):\n",
    "        predictions = classifier(gen_imgs)\n",
    "        p = torch.sum(predictions, dim=0)[1]/torch.sum(predictions)\n",
    "        return lam3*(p*torch.log(2*p) + (1 - p)*torch.log(2*(1 - p)))\n",
    "        \n",
    "        \n",
    "    def forward(self, z):\n",
    "        img = self.model(z)\n",
    "        img = img.view(img.size(0), *img_shape)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "bf2c8595",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(int(np.prod(img_shape)), 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        img_flat = img.view(img.size(0), -1)\n",
    "        validity = self.model(img_flat)\n",
    "\n",
    "        return validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "id": "4484624e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(int(np.prod(img_shape)), 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 2),\n",
    "            nn.Softmax(),\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        img_flat = img.view(img.size(0), -1)\n",
    "        predictions = self.model(img_flat)\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "882b67e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            nn.Linear(int(np.prod(img_shape)), 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(64, 32),\n",
    "        )\n",
    "        \n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            nn.Linear(32, 64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, int(np.prod(img_shape))),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        \n",
    "    def encode(self, x):\n",
    "        x_flat = x.view(x.size(0), -1)\n",
    "        return self.encoder(x_flat)\n",
    "        \n",
    "    def decode(self, encoded):\n",
    "        decode = self.decoder(encoded)\n",
    "        decode = decode.view(decode.size(0), *img_shape)\n",
    "        return decode\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.decode(self.encode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "623dd5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_loss = torch.nn.BCELoss()\n",
    "classifier = Classifier()\n",
    "if cuda:\n",
    "    classifier.cuda()\n",
    "    classifier_loss.cuda()\n",
    "optimizer_C = torch.optim.Adam(classifier.parameters(), lr=lr, betas=(b1, b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "a3a12608",
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_loss = torch.nn.MSELoss()\n",
    "ae = Autoencoder()\n",
    "if cuda:\n",
    "    ae.cuda()\n",
    "    ae_loss.cuda()\n",
    "optimizer_ae = torch.optim.Adam(ae.parameters(), lr=lr, betas=(b1, b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "3a86365c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "adversarial_loss = torch.nn.BCELoss()\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "if cuda:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "    adversarial_loss.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "514253ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "544bd067",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataroot = \"resized_data_mixed\"\n",
    "dataset = dset.ImageFolder(root=dataroot,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Resize(img_size),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                           ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "cb1e4a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = torch.arange(0,1868)\n",
    "dataset1 = data_utils.Subset(dataset, indices)\n",
    "dataloader1 = torch.utils.data.DataLoader(dataset1, batch_size=batch_size,\n",
    "                                         shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "3e3f9230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 0/30] [AE loss: 0.723617]\n",
      "[Epoch 13/200] [Batch 10/30] [AE loss: 0.106132]\n",
      "[Epoch 26/200] [Batch 20/30] [AE loss: 0.068161]\n",
      "[Epoch 40/200] [Batch 0/30] [AE loss: 0.063492]\n",
      "[Epoch 53/200] [Batch 10/30] [AE loss: 0.055330]\n",
      "[Epoch 66/200] [Batch 20/30] [AE loss: 0.055649]\n",
      "[Epoch 80/200] [Batch 0/30] [AE loss: 0.053509]\n",
      "[Epoch 93/200] [Batch 10/30] [AE loss: 0.050813]\n",
      "[Epoch 106/200] [Batch 20/30] [AE loss: 0.049302]\n",
      "[Epoch 120/200] [Batch 0/30] [AE loss: 0.042130]\n",
      "[Epoch 133/200] [Batch 10/30] [AE loss: 0.046514]\n",
      "[Epoch 146/200] [Batch 20/30] [AE loss: 0.039975]\n",
      "[Epoch 160/200] [Batch 0/30] [AE loss: 0.038913]\n",
      "[Epoch 173/200] [Batch 10/30] [AE loss: 0.037278]\n",
      "[Epoch 186/200] [Batch 20/30] [AE loss: 0.035064]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    for i, (imgs, labels) in enumerate(dataloader1):\n",
    "        reconstructed = ae(imgs)\n",
    "        optimizer_ae.zero_grad()\n",
    "        loss = ae_loss(reconstructed, imgs)\n",
    "        loss.backward()\n",
    "        optimizer_ae.step()\n",
    "        batches_done = epoch * len(dataloader1) + i\n",
    "\n",
    "        if batches_done % sample_interval == 0:\n",
    "            print(\n",
    "                \"[Epoch %d/%d] [Batch %d/%d] [AE loss: %f]\"\n",
    "                % (epoch, n_epochs, i, len(dataloader1), loss.item())\n",
    "            )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "4505543d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_sample_apes_encoded = []\n",
    "for i in range(len(dataset1)):\n",
    "    img = dataset[i][0].view(1, *img_shape)\n",
    "    enc = ae.encode(img).detach().view(-1, 32)[0]\n",
    "    image_sample_apes_encoded.append(np.array(enc))\n",
    "image_sample_apes_encoded = np.array(image_sample_apes_encoded).T    \n",
    "image_sample_apes_encoded = torch.tensor(image_sample_apes_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "25c8fd2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1868])"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_sample_apes_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "17c917b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                         shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "08032cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 0/67] [C loss: 0.672675]\n",
      "[Epoch 5/200] [Batch 65/67] [C loss: 0.000021]\n",
      "[Epoch 11/200] [Batch 63/67] [C loss: 0.000011]\n",
      "[Epoch 17/200] [Batch 61/67] [C loss: 0.000001]\n",
      "[Epoch 23/200] [Batch 59/67] [C loss: 0.000001]\n",
      "[Epoch 29/200] [Batch 57/67] [C loss: 0.000001]\n",
      "[Epoch 35/200] [Batch 55/67] [C loss: 0.000001]\n",
      "[Epoch 41/200] [Batch 53/67] [C loss: 0.000000]\n",
      "[Epoch 47/200] [Batch 51/67] [C loss: 0.000000]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs//4):\n",
    "    for i, (imgs, labels) in enumerate(dataloader):\n",
    "        labels = torch.cat((labels.view(len(labels), 1), (1-labels).view(len(labels), 1)), 1).float()\n",
    "        inp = Variable(imgs.type(Tensor))\n",
    "        prediction = classifier(imgs)\n",
    "        lab = Variable(labels.type(Tensor))\n",
    "        optimizer_C.zero_grad()\n",
    "        loss = classifier_loss(prediction, labels)\n",
    "        loss.backward()\n",
    "        optimizer_C.step()\n",
    "        batches_done = epoch * len(dataloader) + i\n",
    "\n",
    "        if batches_done % sample_interval == 0:\n",
    "            print(\n",
    "                \"[Epoch %d/%d] [Batch %d/%d] [C loss: %f]\"\n",
    "                % (epoch, n_epochs, i, len(dataloader), loss.item())\n",
    "            )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "f5a7efa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 0/67] [D loss: 0.675963] [G loss: 0.286307]\n",
      "[Epoch 5/200] [Batch 65/67] [D loss: 0.201108] [G loss: 1.133647]\n",
      "[Epoch 11/200] [Batch 63/67] [D loss: 1.096573] [G loss: -0.074273]\n",
      "[Epoch 17/200] [Batch 61/67] [D loss: 0.296016] [G loss: 0.660062]\n",
      "[Epoch 23/200] [Batch 59/67] [D loss: 0.252975] [G loss: 1.185986]\n",
      "[Epoch 29/200] [Batch 57/67] [D loss: 0.234399] [G loss: 1.028073]\n",
      "[Epoch 35/200] [Batch 55/67] [D loss: 0.168292] [G loss: 2.012831]\n",
      "[Epoch 41/200] [Batch 53/67] [D loss: 0.491887] [G loss: 2.704627]\n",
      "[Epoch 47/200] [Batch 51/67] [D loss: 0.223282] [G loss: 1.491283]\n",
      "[Epoch 53/200] [Batch 49/67] [D loss: 0.176546] [G loss: 1.563449]\n",
      "[Epoch 59/200] [Batch 47/67] [D loss: 0.264487] [G loss: 1.129790]\n",
      "[Epoch 65/200] [Batch 45/67] [D loss: 0.219745] [G loss: 1.551627]\n",
      "[Epoch 71/200] [Batch 43/67] [D loss: 0.212652] [G loss: 1.870407]\n",
      "[Epoch 77/200] [Batch 41/67] [D loss: 0.264425] [G loss: 1.390724]\n",
      "[Epoch 83/200] [Batch 39/67] [D loss: 0.260996] [G loss: 1.850852]\n",
      "[Epoch 89/200] [Batch 37/67] [D loss: 0.228186] [G loss: 2.472866]\n",
      "[Epoch 95/200] [Batch 35/67] [D loss: 0.226715] [G loss: 1.234510]\n",
      "[Epoch 101/200] [Batch 33/67] [D loss: 0.258992] [G loss: 2.139851]\n",
      "[Epoch 107/200] [Batch 31/67] [D loss: 0.237533] [G loss: 1.546523]\n",
      "[Epoch 113/200] [Batch 29/67] [D loss: 0.229002] [G loss: 2.607829]\n",
      "[Epoch 119/200] [Batch 27/67] [D loss: 0.173761] [G loss: 1.633373]\n",
      "[Epoch 125/200] [Batch 25/67] [D loss: 0.229637] [G loss: 1.798326]\n",
      "[Epoch 131/200] [Batch 23/67] [D loss: 0.244541] [G loss: 1.262225]\n",
      "[Epoch 137/200] [Batch 21/67] [D loss: 0.193550] [G loss: 2.062871]\n",
      "[Epoch 143/200] [Batch 19/67] [D loss: 0.187519] [G loss: 1.652016]\n",
      "[Epoch 149/200] [Batch 17/67] [D loss: 0.153155] [G loss: 2.218357]\n",
      "[Epoch 155/200] [Batch 15/67] [D loss: 0.138290] [G loss: 2.369896]\n",
      "[Epoch 161/200] [Batch 13/67] [D loss: 0.155664] [G loss: 2.707213]\n",
      "[Epoch 167/200] [Batch 11/67] [D loss: 0.181184] [G loss: 2.295613]\n",
      "[Epoch 173/200] [Batch 9/67] [D loss: 0.185016] [G loss: 1.905935]\n",
      "[Epoch 179/200] [Batch 7/67] [D loss: 0.142258] [G loss: 2.229219]\n",
      "[Epoch 185/200] [Batch 5/67] [D loss: 0.218275] [G loss: 2.888123]\n",
      "[Epoch 191/200] [Batch 3/67] [D loss: 0.157642] [G loss: 2.040716]\n",
      "[Epoch 197/200] [Batch 1/67] [D loss: 0.164999] [G loss: 2.025087]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    for i, (imgs, labels) in enumerate(dataloader):\n",
    "        # Adversarial ground truths\n",
    "        valid = Variable(Tensor(imgs.size(0), 1).fill_(1.0), requires_grad=False)\n",
    "        fake = Variable(Tensor(imgs.size(0), 1).fill_(0.0), requires_grad=False)\n",
    "\n",
    "        # Configure input\n",
    "        real_imgs = Variable(imgs.type(Tensor))\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Sample noise as generator input\n",
    "        z = Variable(Tensor(np.random.normal(0, 3, (imgs.shape[0], latent_dim))))\n",
    "\n",
    "        # Generate a batch of images\n",
    "        gen_imgs = generator(z)\n",
    "\n",
    "        # Loss measures generator's ability to fool the discriminator\n",
    "        regularize = generator.regularization_fast(gen_imgs, ae)\n",
    "        g_loss = adversarial_loss(discriminator(gen_imgs), valid) + regularize\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Measure discriminator's ability to classify real from generated samples\n",
    "        real_loss = adversarial_loss(discriminator(real_imgs), valid)\n",
    "        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        batches_done = epoch * len(dataloader) + i\n",
    "\n",
    "        if batches_done % sample_interval == 0:\n",
    "            print(\n",
    "                \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
    "                % (epoch, n_epochs, i, len(dataloader), d_loss.item(), g_loss.item())\n",
    "            )\n",
    "            save_image(gen_imgs.data[:25], \"test/%d.png\" % batches_done, nrow=5, normalize=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "63963acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = Variable(Tensor(np.random.normal(0, 3, (imgs.shape[0], latent_dim))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ee95471f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_imgs = generator(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e950b7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb7380716d0>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(  gen_imgs.data[4].permute(1, 2, 0)  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "f044a109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 64, 64])"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_imgs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ab6beefb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7.0000, 6.0000], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(classifier(gen_imgs), dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2e1a43cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6.9441e-01, 3.0559e-01],\n",
       "        [1.0000e+00, 1.3509e-07],\n",
       "        [8.9473e-09, 1.0000e+00],\n",
       "        [1.0000e+00, 1.1838e-06],\n",
       "        [1.0000e+00, 1.2146e-06],\n",
       "        [1.0000e+00, 2.2536e-08],\n",
       "        [5.3620e-08, 1.0000e+00],\n",
       "        [1.0000e+00, 3.8124e-08],\n",
       "        [4.2431e-05, 9.9996e-01],\n",
       "        [3.5550e-08, 1.0000e+00],\n",
       "        [1.0000e+00, 2.0724e-06],\n",
       "        [6.0243e-08, 1.0000e+00],\n",
       "        [1.1111e-07, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(gen_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913648c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
